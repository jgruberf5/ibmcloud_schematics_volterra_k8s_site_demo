#cloud-config
bootcmd:
  - curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add
  - curl -s https://download.docker.com/linux/ubuntu/gpg | apt-key add
  - curl -s https://baltocdn.com/helm/signing.asc | sudo apt-key add
apt:
  sources:
    kubernetes:
      source: "deb http://apt.kubernetes.io/ kubernetes-xenial main"
    docker:
      source: "deb https://download.docker.com/linux/ubuntu focal stable"
    helm:
      source: "deb https://baltocdn.com/helm/stable/debian all main"
package_update: true
package_upgrade: true
packages:
  - apt-transport-https
  - ca-certificates
  - gnupg2
  - software-properties-common
  - bridge-utils
  - curl
  - git
  - wget
  - kubelet
  - kubeadm
  - kubectl
  - containerd.io
  - docker-ce
  - docker-ce-cli
  - helm
write_files:
- owner: root:root
  path: /etc/platform_install.sh
  permissions: '0755'
  content: |
    #!/bin/bash
    apt-mark hold kubelet kubeadm kubectl
    swapoff -a
    modprobe overlay
    modprobe br_netfilter
    sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
    net.bridge.bridge-nf-call-ip6tables = 1
    net.bridge.bridge-nf-call-iptables = 1
    net.ipv4.ip_forward = 1
    EOF
    sudo tee /etc/sysctl.d/hugepages.conf<<EOF
    vm.nr_hugepages=4096
    EOF
    sysctl --system
    mkdir -p /etc/systemd/system/docker.service.d
    sudo tee /etc/docker/daemon.json <<EOF
    {
      "exec-opts": ["native.cgroupdriver=systemd"],
      "log-driver": "json-file",
      "log-opts": {
        "max-size": "100m"
      },
      "storage-driver": "overlay2"
    }
    EOF
    echo 4096 | sudo tee /proc/sys/vm/nr_hugepages
    systemctl daemon-reload 
    systemctl restart docker
    systemctl enable docker
    echo "$(ip route|grep default|grep src|cut -d' ' -f9)    ${instance_name}   ${apifqdn}" >> /etc/hosts
- owner: root:root
  path: /etc/k8s_install.sh
  permissions: '0755'
  content: |
    kubeadm config images pull
    systemctl enable kubelet
    systemctl status kubelet
    # fix /etc/kubernetes/manifests/kube-apiserver.yaml for hostpath
    kubeadm init --pod-network-cidr=${podcidr} --control-plane-endpoint=${apifqdn} --apiserver-advertise-address=$(ip route|grep default|grep src|cut -d' ' -f9)
    mkdir -p /root/.kube
    cp /etc/kubernetes/admin.conf /root/.kube/config
    mkdir -p /home/ubuntu/.kube
    cp /etc/kubernetes/admin.conf /home/ubuntu/.kube/config
    chown -R ubuntu:ubuntu /home/ubuntu/.kube
    export KUBECONFIG=/etc/kubernetes/admin.conf
    kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
    kubectl taint nodes --all node-role.kubernetes.io/master-
    sleep 10
    sed -i 's/apiserver\.key/apiserver\.key\n    - --feature-gates\=RemoveSelfLink\=false/g' /etc/kubernetes/manifests/kube-apiserver.yaml
    kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml
    sleep 10 
    helm repo add rimusz https://charts.rimusz.net
    helm repo update
    helm upgrade --install hostpath-provisioner --namespace kube-system rimusz/hostpath-provisioner
    sleep 10
    kubectl apply -f /etc/volterra-site.yaml
    sleep 10
    kubectl apply -f /etc/container-demo-runner.yaml
- owner: root:root
  path: /etc/volterra-site.yaml
  permissions: '0644'
  content: |
    apiVersion: v1
    kind: Namespace
    metadata:
      name: ves-system
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: volterra-ce-init
      namespace: ves-system
    spec:
      selector:
        matchLabels:
          name: volterra-ce-init
      template:
        metadata:
          labels:
            name: volterra-ce-init
        spec:
          hostNetwork: true
          hostPID: true
          containers:
          - name: volterra-ce-init
            image: docker.io/volterraio/volterra-ce-init
            volumeMounts:
            - name: hostroot
              mountPath: /host
            securityContext:
              privileged: true
          volumes:
          - name: hostroot
            hostPath:
              path: /
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: vpm-sa
      namespace: ves-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: vpm-role
      namespace: ves-system
    rules:
    - apiGroups: ["*"]
      resources: ["*"]
      verbs: ["*"]
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: vpm-role-binding
      namespace: ves-system
    subjects:
    - kind: ServiceAccount
      name: vpm-sa
      apiGroup: ""
      namespace: ves-system
    roleRef:
      kind: Role
      name: vpm-role
      apiGroup: rbac.authorization.k8s.io
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: ver
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
    - kind: ServiceAccount
      name: ver
      namespace: ves-system
    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: vpm-cfg
      namespace: ves-system
    data:
      config.yaml: |
        Vpm:
          ClusterName: ${sitename}
          ClusterType: ce
          Config: /etc/vpm/config.yaml
          DisableModules: ["dictator", "recruiter"]
          Latitude: ${latitude}
          Longitude: ${longitude}
          MauriceEndpoint: https://register.ves.volterra.io
          MauricePrivateEndpoint: https://register-tls.ves.volterra.io
          PrivateNIC: eth0
          SkipStages: ["osSetup", "etcd", "kubelet", "master", "voucher", "workload", "controlWorkload"]
          Token: ${sitetoken}
          CertifiedHardware: k8s-minikube-voltmesh
    ---
    apiVersion: apps/v1
    kind: StatefulSet
    metadata:
      name: vp-manager
      namespace: ves-system
    spec:
      replicas: ${replicas}
      selector:
        matchLabels:
          name: vpm
      serviceName: "vp-manager"
      template:
        metadata:
          labels:
            name: vpm
            statefulset: vp-manager
        spec:
          serviceAccountName: vpm-sa
          initContainers:
          - name : vpm-init-config
            image: busybox
            volumeMounts:
            - name: etcvpm
              mountPath: /etc/vpm
            - name: vpmconfigmap
              mountPath: /tmp/config.yaml
              subPath: config.yaml
            command:
            - "/bin/sh"
            - "-c"
            - "cp /tmp/config.yaml /etc/vpm"
          containers:
          - name: vp-manager
            image: docker.io/volterraio/vpm
            imagePullPolicy: Always
            volumeMounts:
            - name: etcvpm
              mountPath: /etc/vpm
            - name: varvpm
              mountPath: /var/lib/vpm
            - name: podinfo
              mountPath: /etc/podinfo
            - name: data
              mountPath: /data
            securityContext:
              privileged: true
          terminationGracePeriodSeconds: 1
          volumes:
          - name: podinfo
            downwardAPI:
              items:
                - path: "labels"
                  fieldRef:
                    fieldPath: metadata.labels
          - name: vpmconfigmap
            configMap:
              name: vpm-cfg
      volumeClaimTemplates:
      - metadata:
          name: etcvpm
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 1Gi
      - metadata:
          name: varvpm
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 1Gi
      - metadata:
          name: data
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 1Gi
- owner: root:root
  path: /etc/container-demo-runner.yaml
  permissions: '0644'
  content: |
    apiVersion: v1
    kind: Namespace
    metadata:
      name: ${demonamespace}
      labels:
        name: ${demonamespace}
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: diag-container
      namespace: ${demonamespace}
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: cluster-role-binding-clusterrolebinding-simple
    subjects:
      - kind: ServiceAccount
        name: diag-container
        namespace: ${demonamespace}
    roleRef:
      kind: ClusterRole
      name: cluster-role-binding-diag-container
      apiGroup: rbac.authorization.k8s.io
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: cluster-role-binding-diag-container
    rules: 
    - apiGroups: [""] 
      resources: ["pods","services","namespaces","deployments","jobs","statefulsets","persistentvolumeclaims"] 
      verbs: ["get", "watch", "list"]
    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: diag-container-config
      namespace: ${demonamespace}
    data:
      allowed_commands: |
        ["^ping", "^cat /etc/hosts", "^cat /etc/resolv.conf", "^env$", "^ip route$", "^ip addr$", "^ip link$", "^ip neigh", "^netstat", "^dig", "^nc", "^ab", "^siege", "^tcping", "^traceroute", "^tcptraceroute", "^curl", "^whois", "^kubectl", "^iperf", "^sockperf"]
      host_entries: []
      http_listen_address: "0.0.0.0"
      http_listen_port: "8080"
      ws_listen_address: "0.0.0.0"
      ws_listen_port: "8080"
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: diag-container-app
      namespace: ${demonamespace}
    spec:
      selector:
        matchLabels:
          app: diag-container-app
      replicas: 1
      template:
        metadata:
          labels:
            app: diag-container-app
        spec:
          serviceAccountName: diag-container
          volumes:
          - name: diag-container-volume
            configMap:
              name: diag-container-config
          containers:
          - name: diag-container
            image: jgruberf5/container-demo-runner:latest
            ports:
            - containerPort: 8080
              protocol: TCP
            - containerPort: 8080
              protocol: TCP
            - containerPort: 5001
              protocol: TCP
            volumeMounts:
            - name: diag-container-volume
              mountPath: /etc/container-demo-runner
            env:
            - name: BANNER
              value: ${demobanner}
            - name: BANNER_COLOR
              value: "${demobannercolor}"
            - name: BANNER_TEXT_COLOR
              value: "${demobannertextcolor}"
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: diag-container-web-in-cluster
      namespace: ${demonamespace}
    spec:
      type: ClusterIP
      selector:
        app: diag-container-app
      ports:
      - name: http
        port: 8080
        targetPort: 8080
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: diag-container-web
      namespace: ${demonamespace}
    spec:
      type: NodePort
      selector:
        app: diag-container-app
      ports:
      - name: http
        port: 8080
        nodePort: 30080
        targetPort: 8080
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: diag-container-iperf-in-cluster
      namespace: ${demonamespace}
    spec:
      type: ClusterIP
      selector:
        app: diag-container-app
      ports:
      - name: iperf
        port: 5001
        targetPort: 5001
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: diag-container-sockperf-in-cluster
      namespace: ${demonamespace}
    spec:
      type: ClusterIP
      selector:
        app: diag-container-app
      ports:
      - name: sockperf
        port: 11111
        targetPort: 11111
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: diag-container-sockperf
      namespace: ${demonamespace}
    spec:
      type: NodePort
      selector:
        app: diag-container-app
      ports:
      - name: sockperf
        port: 11111
        nodePort: 31111
        targetPort: 11111
runcmd:
  - [ /etc/platform_install.sh ]
  - [ /etc/k8s_install.sh ]